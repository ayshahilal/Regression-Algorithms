# -*- coding: utf-8 -*-
"""AyşeHilalDoğan_YZ_Proje.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WP7hAkAlqEdj_ExRxVMWsH8M49rgdAq8
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries
import pandas as pd
import numpy as np
import math
from math import log
from sklearn import metrics
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import neighbors
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import CategoricalNB
# %matplotlib inline

def load_dataset():
  # read the dataset file (csv)
  kiralik = pd.read_csv('kiralik.csv', delimiter=';')
  
  #print(kiralik.head())
  #print(kiralik.shape)
  #print(kiralik.describe())

  return kiralik

#kiralik = pd.read_csv('kiralik.csv', delimiter=';') 
#kiralik.head()
#kiralik.shape
#kiralik.describe()

def plot(kiralik):
  kiralik.plot(x='fiyat', y='brut', style='o')
  plt.title('fiyat-metrekare')
  plt.xlabel('fiyat')
  plt.ylabel('brut')
  plt.show()

  kiralik.plot(x='fiyat', y='yas', style='o')
  plt.title('fiyat-bina yasi')
  plt.xlabel('fiyat')
  plt.ylabel('yas')
  plt.show()

# Regression Algorithms

def LinearRegressionFunc(X_train, X_test, y_train, y_test):
  regressor = LinearRegression()
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_test)
  df = pd.DataFrame({'Gercek': y_test, 'Tahmin edilen': y_pred})
  print("Gercek - Tahmin Edilen")
  print("")
  print(df)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))

def DecisionTreeFunc(X_train, X_test, y_train, y_test):
  depth = 7
  regressor = DecisionTreeRegressor(max_depth=depth)
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_test)
  df = pd.DataFrame({'Gercek': y_test, 'Tahmin edilen': y_pred})
  print("Gercek - Tahmin Edilen")
  print("")
  print(df)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))

def KNNFunc(X_train, X_test, y_train, y_test):
  n = 7
  regressor = neighbors.KNeighborsRegressor(n)
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_test)
  df = pd.DataFrame({'Gercek': y_test, 'Tahmin edilen': y_pred})
  print(df)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))

def GaussianNaiveBayesFunc(X_train, X_test, y_train, y_test):
  regressor = GaussianNB()
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_test)
  df = pd.DataFrame({'Gercek': y_test, 'Tahmin edilen': y_pred})
  print("Gercek - Tahmin Edilen")
  print("")
  print(df)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))

def MultinomialNaiveBayesFunc(X_train, X_test, y_train, y_test):
  regressor = MultinomialNB()
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_test)
  df = pd.DataFrame({'Gercek': y_test, 'Tahmin edilen': y_pred})
  print(df)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))

def ComplementNaiveBayesFunc(X_train, X_test, y_train, y_test):
  regressor = ComplementNB()
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_test)
  df = pd.DataFrame({'Gercek': y_test, 'Tahmin edilen': y_pred})
  print(df)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))

def BernoulliNaiveBayesFunc(X_train, X_test, y_train, y_test):
  regressor = BernoulliNB()
  regressor.fit(X_train, y_train)
  y_pred = regressor.predict(X_test)
  df = pd.DataFrame({'Gercek': y_test, 'Tahmin edilen': y_pred})
  print(df)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))

# Logistic Regression Algorithm

def cost_function(predicted, y_train, m):
  sum = 0
  for i in range(0, m):
    x = y_train.index[i]
    p1 = log(1 - predicted[i])
    p2 = log(predicted[i])
    sum += (1-x) * p1 + x * p2

  sum = -(1/m) * sum 
  
  return sum

def katsayilari_guncelle(predicted, m, train_data, y_train):
  a = 0.001 #learning rate
  sum= 0
  katsayilar = np.zeros(10)

  # her bir ozelligi guncelle (9 ozellik icin)
  for k in range(0,10):
    for i in range(0,m):
      sum += (predicted[i] - y_train.index[i]) * train_data[i][k]

    katsayilar[k] = predicted[k]- (a/m)*sum

  return katsayilar

def LogisticRegressionFunction(X_train, X_test, y_train, y_test):
  n = 10  #verisetinde 9 ozellik var + 1 ise theta0 icin gerekiyor
  m = X_train.shape[0]  #verisetindeki ornek sayısı
  iteration = True

  katsayilar = np.zeros(n) # baslangicta butun katsayıları 0 ayarla
  # carpim icin trainin basina 1 lerden olusan bir satır eklenir 
  train_data = np.concatenate((np.ones((X_train.shape[0],1)),X_train),axis=1)  

  min = 10000

  # daha fazla kuculmeyene kadar iterasyonu devam ettir 
  while(iteration):
    # katsayilarla train matrisini carp 
    carpim = np.dot(katsayilar, train_data.T)
 
    # belirlenen carpim dizisini fiyat'i tahmin etmek icin kullan 
    # sigmoid fonksiyonu
    predicted = 1/(1 + np.exp( -carpim ))

    # error u bul
    error = cost_function(predicted, y_train, m) 
  
    # katsayıları guncelle (10 luk bir dizi)
    katsayilar = katsayilari_guncelle(predicted, m, train_data, y_train)

    if error < min:
      min = error
    # error bir onceki iterasyonla aynı degerde ise iterasyonu durdur
    if error == min:
      iteration = False
  print("Basari",error)

#Regression main function

def regression(selection):
   # load the dataset 
    kiralik = load_dataset()

    #prepare train data labels
    X=kiralik[['oda','salon','brut','net','yas','kat','esyali_mi','banyo','depozito']]
    # prepare the target
    y=kiralik['fiyat']

    #plot(kiralik)
    
    # Split this data as 1500 training and 500+ test
    X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

    if selection == 1:
      LinearRegressionFunc(X_train, X_test, y_train, y_test)
    elif selection == 2:
      DecisionTreeFunc(X_train, X_test, y_train, y_test)
    elif selection == 3:
      KNNFunc(X_train, X_test, y_train, y_test)
    elif selection == 4:
      GaussianNaiveBayesFunc(X_train, X_test, y_train, y_test)
    elif selection == 5:
      MultinomialNaiveBayesFunc(X_train, X_test, y_train, y_test)
    elif selection == 6:
      ComplementNaiveBayesFunc(X_train, X_test, y_train, y_test)
    elif selection == 7:
      BernoulliNaiveBayesFunc(X_train, X_test, y_train, y_test)
    elif selection == 8:
      LogisticRegressionFunction(X_train, X_test, y_train, y_test)

def main():
    selection = 1 #Linear Regression
    #selection = 2 #Decision Tree
    #selection = 3 #KNN
    #selection = 4 #Gaussian Naive Bayes
    #selection = 5 #Multinomial Naive Bayes
    #selection = 6 #Complement Naive Bayes
    #selection = 7 #Bernoulli Naive Bayes
    #selection = 8 #Logistic Regression (my algorithm)

    regression(selection)
   
if __name__ == "__main__":
  main()
